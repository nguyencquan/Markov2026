\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage[margin=.5in]{geometry}
\setlength{\parindent}{0pt}

\title{Homework 1}
\author{Quan Nguyen}
\date{January 2026}

\begin{document}

\maketitle
See https://github.com/nguyencquan/Markov2026 for code used in simulations.

\section{Power Law Sampling}
Assume we have some distribution
\begin{equation}
    \begin{cases}
    Cx^{-\gamma}& x\geq x_0,\\
    0, & x<x_0
\end{cases}
\end{equation}
\textbf{a)}
Where $x_0>0$ and $\gamma > 3$

A valid pdf must have a total probability of 1, which is calculated below:
\begin{equation}
    1 = \int_{-\infty}^{\infty}Cx^{-\gamma}dx = C\int_{x_0}^{\infty}x^{-\gamma}dx
    =\frac{C}{x^{\gamma-1}(1-\gamma)}\vert_{x_0}^\infty = \frac{C}{x_0^{\gamma-1}(\gamma-1)}
\end{equation}
\begin{equation}
    C = x_0^{\gamma-1}(\gamma-1)\\
\end{equation}

\textbf{b)}
First calculating the cummulative distribution we take a step from equation 2 except take the integral from
$x_0$ to $x$ instead.

\begin{equation}
    1 = \int_{-\infty}^{\infty}Cx^{-\gamma}dx = C\int_{x_0}^{x}x^{-\gamma}dx
    =\frac{C}{x^{\gamma-1}(1-\gamma)}\vert_{x_0}^x = \frac{C}{x_0^{\gamma-1}(\gamma-1)} + \frac{C}{x^{\gamma-1}(1-\gamma)}
\end{equation}
Substituting in C
\begin{equation}
    F(x) = 
    \begin{cases}
        1 - \frac{x_0^{\gamma-1}}{x^{\gamma-1}}, & x \geq x_0\\
        0, & x< x_0
    \end{cases}
\end{equation}

To calculate the inverse, note we can do root math since $x_0>0$
\begin{align}
    p &= 1 - \frac{x_0^{\gamma-1}}{x^{\gamma-1}}\\
    \frac{x_0^{\gamma-1}}{x^{\gamma-1}} &= 1-p\\
    \frac{x_0}{x} &= (1-p)^\frac{1}{\gamma-1}\\
    x &= \frac{x_0}{(1-p)^\frac{1}{\gamma-1}}
\end{align}

\textbf{c)} Also we can note that we can use $U(0,1)$ without $1-p$ to speed it up since their distribution is
exactly the same.


1. Sample from a normal distribution $U(0,1)$

2. substitue value into p for $\frac{x_0}{p^\frac{1}{\gamma-1}}$
\newpage
\textbf{d)}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\linewidth]{100Samples.png}
    \caption{Graph of pdf and random sampling of 100 points, given $\gamma = 4$ and $x_0 = 10$}
\end{figure}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\linewidth]{1000Samples.png}
    \caption{Graph of pdf and random sampling of 1000 points, given $\gamma = 4$ and $x_0 = 10$}
\end{figure}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\linewidth]{10000Samples.png}
    \caption{Graph of pdf and random sampling of 10000 points, given $\gamma = 4$ and $x_0 = 10$}
\end{figure}


\clearpage

\section{Gamma Sampling}

Suppose we have $X\sim Gamma(2,1)$ with pdf
\begin{equation}
    f(x) = \begin{cases}
        x e^{-x}, & x > 0\\
        0, & x \leq 0
    \end{cases}
\end{equation}
\textbf{a)} To calculate the CDF we take the integral of the pdf from 0 to x.
Using integration by part with $u = x$ and $dv = e^{-x}$
\begin{align}
    \int_{-\infty}^{x}f(x)dx = \int_0^x xe^{-x}dx &=-xe^{-x}+\int_{0}^{x}e^{-x}dx\\
    &=-e^{-x}(x+1)|_{0}^x\\
    &=1-e^{-x}(x+1)
\end{align}
Hence we have
\begin{equation}
    F(x) = 
    \begin{cases}
        1-e^{-x}(x+1), & x> 0\\
        0, & x\leq 0
    \end{cases}
\end{equation}
\textbf{b}
I will be using Newtons method to find the inverse of the CDF. The method will guess a root at one and runs 5
times then repeated until the root is positive. There is a sanity check where if the root stays negative
an error occurs. Fortunately the code works. The solution to the CDF will be calcualted by solving the root of $F(x)-u = 0$.
The function took 5.1287 seconds to run.

\textbf{c}
To solve for the optimal value for c, I will use the inequality:

\begin{align}
    C\frac12e^{-\frac{x}{2}}&\geq xe^{-x}\\
    C & \geq 2 xe^{\frac{-x}{2}} = h(x)\\
\end{align}

To solve for the critical point of the equation, take the derivative and solve 
for the roots 
\begin{align}
    h'(x) &= 2e^{\frac{-x}{2}} - xe^{\frac{-x}{2}}\\
    &= e^{\frac{-x}{2}}(2-x) = 0\\
\end{align}

Since the derivative is a strictly decreasing function and is positive when $x<2$, the critical value
is the point where $h(x)$ is the absolute maximum yielding
\begin{equation}
    C \geq h(2) = 4/e
\end{equation}

Since efficiency is determined by $1/C$ the smallest possible value of C to maximize efficiency is
$4/e$.

Also note that the given distribution $g(x)$ is aa pdf for an exponential distribution with $\lambda = 1/2$
Using an acceptace rejection algorithm, it took 3.1873 seconds to complete.

\textbf{d} 
After running the simulation code, the experiment was completed in 0.0872 seconds.

\textbf{f} The fastest algorithm is summing the exponential distributions, then it is the
acceptance-rejection criterion, and the slowest is using the root solver. Note the root solver
doesn't even give very accurate results and takes even more time if I want it to be more accurate.

\textbf{g}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\linewidth]{pdfAcceptReject.png}
    \caption{Graph for the pdf of $xe^{-x}$ using acceptance rejection criterion.}
\end{figure}

\section{Mixture Sampling to Polynomial Sampling}

\textbf{a} To show that these two distributions are the same, we will be calculating the CDF
of Y. Note the main proof follows the law of total probability:
\begin{align}
    Y(y) = P(Y\leq y) &= \sum_{j = 1}^N P(X_J \leq y| J = j)P(J = j)\\
    &= \sum_{j=1}^{N} P(X_j\leq y)P(J=j)\\
    &= \sum_{j=1}^{N} P(X_j\leq y)a_j\\
    &= \sum_{j=1}^{N}\int_0^y f_j(x)dx a_j\\
    &= \sum_{j=1}^{N}\int_0^y f_j(x)a_jdx\\
    &= \int_{0}^{y}\sum_{j=1}^{N} a_jf_j(x)dx\\
    &= \int_{0}^y f(x)dx = F(y)
\end{align}

\textbf{b} To solve for the CDF, I will be taking the integral of $f_j$ from 0 to x.
\begin{equation}
    \int_0^x f_j(x)dx = \int_{0}^{x} (j+1)x^jdx = =x^{j+1}|_0^x = x^{j+1}
\end{equation}

To sample from this distribution I will use inverse sampling. The inverse of the CDF is $x = u^\frac{1}{j+1}$.
Hence the pseudo code will be.

1. Generate a random value $u\sim U(0,1)$

2. return $u^\frac{1}{j+1}$


\textbf{c} I will be finding a way to setup $f(z)$ Using the sum that is given above. Since
the highest power of $f(z)$ is $5$, $N = 5$ resulting in the following sum:
\begin{equation}
    \sum_{j=1}^5a_jf_j(z) = a_12z + a_23z^2 + a_34z^3 + a_45z^4 + a_56z^5
\end{equation}
Furthermore $a_1 = \frac12, a_5 = \frac12$ while all other $a_j$ equal 0. Using
the fact from question a due to the law of total probability, we can do conditional sampling.

1. Sample $j\sim U(0,1)$

2. if $j \geq .5$ $j=1$ else $j=5$

3. generate $u\sim U(0,1)$

4. return $u^\frac{1}{j+1}$

\section{Gaussian Sampling}
The pdf of a standard normal distribution is shown below.
\begin{equation}
    f(x) = \frac{1}{\sqrt{2\pi}}e^\frac{-x^2}{2}
\end{equation}
Note it is not possible to conver this integral to a CDF. Hence Acceptance-rejection will be used.
I will be using the pmf $g(x)$ of an exponential distribution  with $\lambda = 1$ since 
\begin{align}
    cg(x) &\geq f(x)\\
    ce^{-x} &\geq \frac1{\sqrt{2\pi}}e^\frac{-x^2}{2}\\
    c\sqrt{2\pi} e^{-x} &\geq e^\frac{-x^2}{2}\\
    c\sqrt{2\pi} e^{\frac{x^2-2x}{2}} &\geq 1\\
    c\sqrt{2\pi} e^{\frac{x(x-2)}{2}} &\geq 1\\
\end{align}
Note how there are two critical points for $(x(x-2))$ at $x=0,2$. By taking the derivative,
there is one critical point at at $x=1$ and it is the absolute minimum as this is a concave up
quadratic curve. Hence the value of x that would minimize the left side of the inequality would be
$x=2$ yielding $\sqrt{2\pi}e^{-1/2} = .6577$. However we can set c to $\sqrt{\frac{e}{2\pi}} = 1.5205$
to satisfy the inequality. Also note the gaussian distribution is symmetrical hence:

1. Choose a random $x\sim Exp(1)$

2. Choose a random $u \sim U(0,1)$

3. if $u\geq \frac1{\sqrt{2\pi}}e^\frac{-x^2}{2}/exp(\frac{x^2-2x-1}{2})$ continue to 4 else go back to 1

4. Choose a random $l \sim U(0,1)$

5. If $l \geq .5$ return x else return -x

\section{Discrete Random Variable}

\textbf{a} To write the cummulative distributions we start with the probabiliy distribution function:
\begin{align}
    P(1) &= .01\\
    P(2) &= .9\\
    P(3) &= .09\\
\end{align}
Hence:

\begin{align}
    P(X \leq 1) &= .01\\
    P(X \leq 2) &= .91\\
    P(X\leq 3) &= 1\\
\end{align}
Note however we can invert the equations above and use the uniform distribution to generate discrete sampling

1. Sample $u = U(0,1)$

2. if $u \leq .01$ return 1 else

3. if $u \leq .91$ return 2

4. else return 3

\textbf{b} If we arrange them in order of the highest probabilities this would improve the
efficiency of the code since the ones that more likely to be within threshold are checked first.
The best order is $\{2,3,1\}$. Using the law of total expectation for the first algorithm, where T is the number
of comparisons not including a standalone else:
\begin{equation}
    E[T] = E[E[T|X]] = \sum_{i=1}^3 E[T|X=i]P(X=i) = 1 * .01 + 2 *.9 + 2*.09 = 1.99
\end{equation}
While the new algorithm has an expected comparison count of:
\begin{equation}
    E[T] = E[E[T|X]] = \sum_{i=1}^3 E[T|X=i]P(X=i) = 1 * .9 + 2 *.09 + 2*.01 = 1.1
\end{equation}

Therefore, if we run the each algorithm for M samples, the first method would require $1.99M$ comparisons
while the second method would only require $1.1M$ comparisons.
\end{document}